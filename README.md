# Tetrimax

The goal of this project is to generate an agent to play tetris.  
We explored a variety of ways to generate agents. These include manmade agents, a homemade genetic algorithm and utilizing python-neat. The goal of each agent is to generate an evaluation score for a given tetris board. Then each agent generates all possible moves given the current piece and then generates the boards for that piece. From there we ask the agent to weight each board and select the move that corresponds to the highest board rating.

This basic interface allows us to easily generate many agents that operate in the same way, but have different evaluation tactics.

The core of the project is the evaluation engine. This engine takes in an agent and simulates an entire game of tetris. It has a variety of tunable parameters and operates by allowing agents to place the current piece in any valid position. Notably, unlike a human player, the agent is not moving the piece itself. As all of our agents are simply computers, there was no real reason to force the agents to learn to perform complex input techniques. The hard part of the game is to choose where to put the piece.

The core of the engine is located in piece.py, tetrisPieceGenerator.py, tetrisUtilities.py, tetrisClasses.py, and tetrisSimulation.py. There are also a variety of agents defined. Most of the agents take in a feature generation function as part of their init. This is so that an agent is defined by its behavior and not simply the way its featurizing a given board.

There are three main agents that showed promise given testing. Two different agents tuned with a manual genetic algorithm (found in geneticFactory.py). One of them was a simple linear agent which had as many weights as features. The output of a given board state is simply given by x1w1 + x2w2 + ... + xnwn. Another was constructed with a fixed size neural network. This network was constructed with two hidden layers both with a number of perceptrons equal to the number of features. Finally an agent was trained using NEAT (NeuroEvolution of Augmenting Topologies) using the neat-python module.

Agents were trained under two main goals: to survive forever and to score the most points in a fixed length game. How each agents goal was changed vastly impacted the way in which the agent behaved. Agents whose goal was simply to score as many lines as possible took no risk and cleared lines as fast as possible. Their boards rarely were higher than the 3rd or 4th row. Based on our testing, these agents would never lose. They would easily clear thousands of lines without breaking a sweat and never coming close to losing. On the other hand, agents who were given a goal of scoring the most points (points here are detirmined as given in the standard rules of tetris) often intentionally did not clear rows with the intent to score a tetris at some point in the future. This was awesome to see. Our agent successfully played tetris and were playing as human players did. Our agents scored about a 30% tetis rate. This is lower than the percentage that top human players are able to achieve, but they are able to perform at that level while never losing.

There are two reasons I belive that our agents are unable to play perfectly (or at least to the level of high human players). The first one is that they do not have perfect information. At the moment they are learning based on a feature representation of the board. This means that they don't even get to see the whole board all at once. Our agents are learning based what we think they should find important, but if they were allowed to learn themselves that would be interesting. Secondly, the agents are not given information about the pieces that have come and gone during the time they were playing. This means that an agent does not know what the max time until a hero (4x1) piece is. Human players keep track of the piece count aggresively so that they know how risky they should be playing in the moment or at least so they know what to expect in the future. They can prepare based on their memory. As of right now our agents do not have that memory.
